
# ğŸ›’ ClasificaciÃ³n Multiclase de Productos de Supermercado  

## ğŸ¯ Objetivo  
Entrenar un modelo de red neuronal para clasificar imÃ¡genes de productos de supermercado en **53 clases** diferentes, optimizando la precisiÃ³n y la generalizaciÃ³n incluso con un dataset reducido y clases desbalanceadas.  

---

## ğŸ“‚ Dataset y Preprocesamiento  
- Estructura esperada:  
```
data/items_prepared/{train,val,test}/{clase}/imagen.jpg
```  
- Las imÃ¡genes se reorganizan por clase en `paths.prepared_data` (definido en `config.yaml`).  
- ResoluciÃ³n utilizada: **128Ã—128** pÃ­xeles.  
- NormalizaciÃ³n: `preprocess_input` (MobileNetV2) para compatibilidad con pesos preentrenados.  

---

## ğŸ“ˆ EvoluciÃ³n de MÃ©tricas

| VersiÃ³n | DescripciÃ³n | Accuracy | Macro F1 | Weighted F1 |
|---------|-------------|----------|----------|-------------|
| **v1** | CNN simple + `rescale=1./255` | 0.23 | 0.18 | 0.21 |
| **v2** | CNN simple + `class_weight` | ~0.05 | â€” | â€” |
| **v3** | **MobileNetV2 congelada** + `preprocess_input` + Dropout 0.3 | **0.755** | **0.679** | **0.739** |

**Lectura rÃ¡pida:**  
- El cambio a **transfer learning** con MobileNetV2 fue clave (+52 pts en accuracy).  
- `class_weight` no mejorÃ³ la CNN simple debido a la arquitectura limitada y dataset reducido.  
- El modelo final generaliza mejor y mantiene buen balance entre clases.  

---

## âš™ï¸ ConfiguraciÃ³n Final  
- **Modelo:** MobileNetV2 (`weights=imagenet`) congelada + GlobalAveragePooling + Dropout 0.3 + Dense Softmax  
- **Input:** 128Ã—128 px, RGB  
- **Batch size:** 32  
- **Callbacks:** EarlyStopping (patience=8), ReduceLROnPlateau, ModelCheckpoint  
- **Balanceo:** sin `class_weight` (la mejora provino del transfer learning)  

---

## ğŸ“Š Resultados Finales (ValidaciÃ³n)  
| MÃ©trica | Valor |
|---------|-------|
| Accuracy | **0.755** |
| Macro F1 | **0.679** |
| Weighted F1 | **0.739** |

**Hallazgos:**  
- Buen rendimiento global, con mejoras amplias en envases y hortalizas.  
- Persisten 3â€“5 clases con desempeÃ±o bajo (p.ej., algunas variantes de yogur/crema y melÃ³n).  
- El modelo es **presentable para entrega y demostraciÃ³n**.  

---

## ğŸ” Reproducibilidad  

### 1. Preprocesamiento  
- Definir rutas y parÃ¡metros en `config.yaml`:  
```yaml
paths:
  prepared_data: "C:/Users/USS/DeepNeuralNetworkUSS/Projects/Supermarket/data/items_prepared"
  model: "models/cnn_multiclass_v2.keras"
image:
  height: 128
  width: 128
training:
  batch_size: 32
  patience: 8
```

### 2. Entrenamiento (`RedNeuronal.ipynb`)  
```python
train_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    rotation_range=15, width_shift_range=0.1, height_shift_range=0.1,
    shear_range=0.1, zoom_range=0.15, horizontal_flip=True, fill_mode="nearest"
)
val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
cnn_model = model.build_model_transfer(config, num_classes)
cnn_model.fit(train_generator, validation_data=val_generator, callbacks=callbacks)
```

### 3. EvaluaciÃ³n (`Evaluacion.ipynb`)  
- Cargar modelo desde `paths.model`  
- Calcular `accuracy`, `macro-F1`, `weighted-F1`  
- Generar matriz de confusiÃ³n y reporte de clasificaciÃ³n  
- Guardar mÃ©tricas en `results/metrics_v3.yaml` (opcional)  

---

## ğŸ“Œ Limitaciones  
- Variabilidad de iluminaciÃ³n/envase afecta algunas clases.  
- Dataset reducido en ciertas clases (<6 imÃ¡genes en train) â†’ recall bajo.  

---

## ğŸ§­ PrÃ³ximos pasos (opcionales)  
1. **Fine-tuning parcial** del backbone (Ãºltimas ~40 capas, LR=1e-4).  
2. Aumentar resoluciÃ³n a **160Ã—160** si el hardware lo permite.  
3. Augmentations dirigidas (brillo, contraste, zoom) en clases problemÃ¡ticas.  
4. Oversampling ligero en clases con muy pocos ejemplos.  
